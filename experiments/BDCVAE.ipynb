{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bayesian VAE",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPoOTkZVMzZj",
        "outputId": "d2808cd7-96c1-4153-bce4-82966dcd929f"
      },
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "def RMSE(ratings_pred, ratings):\n",
        "    ratings_mask = ratings > 0\n",
        "    return torch.sum((ratings_pred * ratings_mask - ratings) ** 2)\n",
        "\n",
        "# Load data into memory\n",
        "\n",
        "df_train = pd.read_csv('movielens1m_test.csv')\n",
        "df_test = pd.read_csv('movielens1m_train.csv')\n",
        "df_train['UserID'] = df_train['UserID']\n",
        "df_test['UserID'] = df_test['UserID']\n",
        "USERS = max(df_train['UserID'].max(), df_test['UserID'].max()) + 1\n",
        "df_train['MovieID'] = df_train['MovieID']\n",
        "df_test['MovieID'] = df_test['MovieID']\n",
        "MOVIES = max(df_train['MovieID'].max(), df_test['MovieID'].max()) + 1\n",
        "\n",
        "df = pd.concat([df_train, df_test])\n",
        "ROWS = df['UserID']\n",
        "COLS = df['MovieID']\n",
        "\n",
        "print('Finished feature engineering...')\n",
        "print('df shape', len(df))\n",
        "print('df_train.head()', len(df_train))\n",
        "print(df_train.head())\n",
        "print('df_test.head()', len(df_test))\n",
        "print(df_test.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished feature engineering...\n",
            "df shape 1000209\n",
            "df_train.head() 199828\n",
            "   Unnamed: 0  UserID  MovieID  Rating  Timestamp\n",
            "0           3       0        3       4  978300275\n",
            "1          22       0       22       5  978300055\n",
            "2          30       0       30       4  978824291\n",
            "3          34       0       34       4  978824330\n",
            "4          42       0       42       4  978301753\n",
            "df_test.head() 800381\n",
            "   Unnamed: 0  UserID  MovieID  Rating  Timestamp\n",
            "0           0       0        0       5  978300760\n",
            "1           1       0        1       3  978302109\n",
            "2           2       0        2       3  978301968\n",
            "3           4       0        4       5  978824291\n",
            "4           5       0        5       3  978302268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DLhIPWCM7tN",
        "outputId": "4b03ab17-88eb-4c6a-894c-989558842d13"
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed = 1337\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n",
        "print('Using device', device)\n",
        "torch.set_default_tensor_type(torch.FloatTensor)\n",
        "\n",
        "# Construct the train and test rating matrices.\n",
        "\n",
        "train_data = csr_matrix((df_train['Rating'], (df_train['UserID'], df_train['MovieID'])), shape=(USERS, MOVIES), dtype=np.float32)\n",
        "RMSE_train = torch.Tensor(train_data.copy().toarray()).to(device)\n",
        "rows, cols = train_data.nonzero()\n",
        "train_mn = train_data.min()\n",
        "train_mx = train_data.max()\n",
        "for i, j in zip(rows, cols):\n",
        "  train_data[i, j] = (train_data[i, j] - train_mn) / (train_mx - train_mn)\n",
        "\n",
        "\n",
        "test_data = csr_matrix((df_test['Rating'], (df_test['UserID'], df_test['MovieID'])), shape=(USERS, MOVIES), dtype=np.float32)\n",
        "RMSE_test = torch.Tensor(test_data.copy().toarray()).to(device)\n",
        "rows, cols = test_data.nonzero()\n",
        "test_mn = test_data.min()\n",
        "test_mx = test_data.max()\n",
        "for i, j in zip(rows, cols):\n",
        "  test_data[i, j] = (test_data[i, j] - test_mn) / (test_mx - test_mn)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KH-VU_rf_Ga",
        "outputId": "25e404ad-34e6-4105-f5a6-3d1d358963ff"
      },
      "source": [
        "print(train_data.shape, test_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6040, 3706) (6040, 3706)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3gmVid6OGeA",
        "outputId": "70de5654-9813-44d2-ef09-b16d205e37c5"
      },
      "source": [
        "# Precalculate batches for faster computing.\n",
        "R_train = train_data\n",
        "R_train_tensor = []\n",
        "for i in range(USERS):\n",
        "    batch = R_train[i].nonzero()[1]\n",
        "    ts = torch.from_numpy(R_train[i, batch].todense().transpose().astype(np.float32)).to(device)\n",
        "    R_train_tensor.append(ts)\n",
        "print(len(R_train_tensor), R_train_tensor[0].shape)\n",
        "print(R_train.shape)\n",
        "\n",
        "R_train_T = train_data.transpose()\n",
        "R_train_tensor_T = []\n",
        "for j in range(MOVIES):\n",
        "    batch = R_train_T[j].nonzero()[1]\n",
        "    ts = torch.from_numpy(R_train_T[j, batch].todense().transpose().astype(np.float32)).to(device)\n",
        "    R_train_tensor_T.append(ts)\n",
        "print(len(R_train_tensor_T), R_train_tensor_T[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6040 torch.Size([5, 1])\n",
            "(6040, 3706)\n",
            "3706 torch.Size([342, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA-s5UyLycii"
      },
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "from torch.autograd import Variable as V\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x.mul(torch.sigmoid(x))\n",
        "\n",
        "\n",
        "def log_norm_pdf(x, mu, logvar):\n",
        "    return -0.5 * (logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
        "\n",
        "\n",
        "def reparameterize(mu, logvar):\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    return eps.mul(std).add_(mu)\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    pass\n",
        "    # if isinstance(m, nn.Linear):\n",
        "    #     torch.nn.init.normal_(m.weight, 0, 0.01)\n",
        "    #     torch.nn.init.normal(m.bias, 0, 0.01)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.act = nn.LeakyReLU()\n",
        "        self.fc1 = nn.Linear(input_dim, 40)\n",
        "        self.ln1 = nn.LayerNorm(40, eps=eps)\n",
        "\n",
        "        self.fc_mu = nn.Linear(40, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(40, latent_dim)\n",
        "\n",
        "        self.fc1.apply(weights_init)\n",
        "        self.fc_mu.apply(weights_init)\n",
        "        self.fc_logvar.apply(weights_init)\n",
        "\n",
        "    def forward(self, x, dropout_rate, calculate_loss=True):\n",
        "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
        "\n",
        "        h1 = self.ln1(self.act(self.fc1(x)))\n",
        "\n",
        "        mu, logvar = self.fc_mu(h1), self.fc_logvar(h1)\n",
        "\n",
        "        return mu, logvar\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.act = nn.LeakyReLU()\n",
        "        self.fc1 = nn.Linear(latent_dim, input_dim)\n",
        "\n",
        "        self.fc1.apply(weights_init)\n",
        "\n",
        "    def forward(self, z, calculate_loss=True):\n",
        "        x = torch.sigmoid(self.fc1(z))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, device):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
        "        self.decoder = Decoder(hidden_dim, latent_dim, input_dim)\n",
        "        self.zr = torch.zeros(1, latent_dim).to(device=device)\n",
        "        self.mones = torch.ones(1, latent_dim).to(device) * -1\n",
        "\n",
        "    def forward(self, user_ratings, alpha=0.5, beta=None, gamma=1, dropout_rate=0.5, calculate_loss=True, n_epoch=1):\n",
        "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate, calculate_loss=calculate_loss)\n",
        "        z = reparameterize(mu, logvar)\n",
        "        x_pred = self.decoder(z, calculate_loss=calculate_loss)\n",
        "\n",
        "        if calculate_loss:\n",
        "            if gamma:\n",
        "                kl_weight = gamma * 10\n",
        "            elif beta:\n",
        "                kl_weight = beta\n",
        "\n",
        "            # Choose between MSE and cross-entropy\n",
        "\n",
        "            # user_mask = user_ratings > 0\n",
        "            # mll = torch.pow(x_pred * user_mask - user_ratings, 2).sum(dim=-1).mul(kl_weight).mean()\n",
        "            mll = (-torch.log(x_pred) * user_ratings).sum(dim=-1).mul(kl_weight).mean()\n",
        "\n",
        "            # Compute divergence\n",
        "            prior = log_norm_pdf(z, self.zr, self.zr)\n",
        "            kld = (log_norm_pdf(z, mu, logvar) - prior).sum(dim=-1).mul(kl_weight).mean()\n",
        "\n",
        "            negative_elbo = mll + kld\n",
        "\n",
        "            return mu, z, negative_elbo\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def update_prior(self):\n",
        "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))\n",
        "\n",
        "\n",
        "class DualVAE(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim_users, input_dim_movies, device, eps=1e-1):\n",
        "        super(DualVAE, self).__init__()\n",
        "        self.device = device\n",
        "        self.movies_VAE = VAE(hidden_dim, latent_dim, input_dim_users, device)\n",
        "        self.users_VAE = VAE(hidden_dim, latent_dim, input_dim_movies, device)\n",
        "        self.input_dim_users = input_dim_users\n",
        "        self.input_dim_movies = input_dim_movies\n",
        "        self.f = latent_dim\n",
        "        self.f_eye = torch.eye(self.f).to(device)\n",
        "        self.lambda_u = 10\n",
        "        self.lambda_v = 10\n",
        "        mU = MultivariateNormal(torch.zeros(latent_dim), 1 / self.lambda_u * torch.eye(latent_dim))\n",
        "        mV = MultivariateNormal(torch.zeros(latent_dim), 1 / self.lambda_v * torch.eye(latent_dim))\n",
        "        self.U = torch.stack([mU.sample() for _ in range(input_dim_users)]).to(device)\n",
        "        self.V = torch.stack([mV.sample() for _ in range(input_dim_movies)]).to(device)\n",
        "\n",
        "    def pmf_init(self, R_train, R_train_tensor, R_train_T, R_train_tensor_T):\n",
        "        # Compute initial U vector\n",
        "        self.v_conv_all = torch.stack([torch.einsum('n,m->nm', vj, vj.T) for vj in self.V])\n",
        "\n",
        "        self.u_dev = torch.zeros((self.input_dim_users, self.f, self.f)).to(self.device)\n",
        "        for i in range(self.input_dim_users):\n",
        "            batch = torch.from_numpy(R_train[i].nonzero()[1]).to(dtype=torch.int64)\n",
        "            v_sum = torch.sum(self.v_conv_all[batch], axis=0)\n",
        "            self.u_dev[i] = torch.inverse(v_sum + self.lambda_u * self.f_eye)\n",
        "\n",
        "            v_conv = torch.sum(R_train_tensor[i] * self.V[batch], axis=0)\n",
        "            self.U[i] = self.u_dev[i] @ v_conv\n",
        "\n",
        "        # Compute initial V vector\n",
        "        self.u_conv_all = torch.stack([torch.einsum('n,m->nm', ui, ui.T) for ui in self.U])\n",
        "\n",
        "        self.v_dev = torch.zeros((self.input_dim_movies, self.f, self.f)).to(self.device)\n",
        "        for j in range(self.input_dim_movies):\n",
        "            batch = torch.from_numpy(R_train_T[j].nonzero()[1]).to(dtype=torch.int64)\n",
        "            u_sum = torch.sum(self.u_conv_all[batch], axis=0)\n",
        "            self.v_dev[j] = torch.inverse(u_sum + self.lambda_v * self.f_eye)\n",
        "\n",
        "            u_conv = torch.sum(R_train_tensor_T[j] * self.U[batch], axis=0)\n",
        "            self.V[j] = self.v_dev[j] @ u_conv\n",
        "\n",
        "    def forward(self, ratings, R_train, R_train_tensor, R_train_T, R_train_tensor_T, userFeatures=None, beta=None, gamma=1, dropout_rate=0.5, calculate_loss=True, n_epoch=1):\n",
        "        if calculate_loss:\n",
        "            mu_users, z_i, user_loss = self.users_VAE(ratings, beta=beta, gamma=gamma, dropout_rate=dropout_rate,\n",
        "                                                      calculate_loss=calculate_loss, n_epoch=n_epoch)\n",
        "            mu_movies, z_j, movie_loss = self.movies_VAE(ratings.T, beta=beta, gamma=gamma, dropout_rate=dropout_rate,\n",
        "                                                         calculate_loss=calculate_loss, n_epoch=n_epoch)\n",
        "            self.pmf_estimate(mu_users, mu_movies, R_train, R_train_tensor, R_train_T, R_train_tensor_T)\n",
        "            # y_hat = self.U @ self.V.T\n",
        "\n",
        "            normalizer = 0.5 * self.lambda_u * torch.sum(\n",
        "                self.u_dev * torch.pow(self.U - mu_users, 2).unsqueeze(2)) + 0.5 * self.lambda_v * torch.sum(\n",
        "                self.v_dev * torch.pow(self.V - mu_movies, 2).unsqueeze(2))\n",
        "            total_loss = user_loss + movie_loss + normalizer\n",
        "            return total_loss, user_loss, movie_loss, normalizer\n",
        "\n",
        "        if userFeatures is not None:\n",
        "            mu_users = self.users_VAE(userFeatures, beta=beta, gamma=gamma, dropout_rate=dropout_rate,\n",
        "                                      calculate_loss=calculate_loss, n_epoch=n_epoch)\n",
        "            y_hat = mu_users @ self.V.T\n",
        "        else:\n",
        "            y_hat = self.U @ self.V.T\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "    def pmf_estimate(self, mu_users, mu_movies, R_train, R_train_tensor, R_train_T, R_train_tensor_T):\n",
        "        with torch.no_grad():\n",
        "            # Update U vector\n",
        "            self.v_conv_all = torch.stack([torch.einsum('n,m->nm', vj, vj.T) for vj in self.V])\n",
        "            for i in range(self.input_dim_users):\n",
        "                batch = torch.from_numpy(R_train[i].nonzero()[1]).to(dtype=torch.int64)\n",
        "                v_sum = torch.sum(self.v_conv_all[batch], axis=0)\n",
        "                v_dev_sm = torch.sum(self.v_dev[batch], axis=0)\n",
        "                self.u_dev.data[i] = torch.inverse(v_sum + v_dev_sm + self.lambda_u * self.f_eye)\n",
        "\n",
        "                v_conv = torch.sum(R_train_tensor[i] * self.V[batch], axis=0)\n",
        "                self.U.data[i] = self.u_dev[i] @ (v_conv + self.lambda_u * mu_users[i])\n",
        "\n",
        "            # Update V vector\n",
        "            self.u_conv_all = torch.stack([torch.einsum('n,m->nm', ui, ui.T) for ui in self.U])\n",
        "            for j in range(self.input_dim_movies):\n",
        "                batch = torch.from_numpy(R_train_T[j].nonzero()[1]).to(dtype=torch.int64)\n",
        "                u_sum = torch.sum(self.u_conv_all[batch], axis=0)\n",
        "                u_dev_sm = torch.sum(self.u_dev[batch], axis=0)\n",
        "                self.v_dev.data[j] = torch.inverse(u_sum + u_dev_sm + self.lambda_v * self.f_eye)\n",
        "\n",
        "                u_conv = torch.sum(R_train_tensor_T[j] * self.U[batch], axis=0)\n",
        "                self.V.data[j] = self.v_dev[j] @ (u_conv + self.lambda_v * mu_movies[j])\n",
        "\n",
        "    def update_prior(self):\n",
        "        self.users_VAE.update_prior()\n",
        "        self.movies_VAE.update_prior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "WcELyxrhy0Nj",
        "outputId": "ed4a3e82-a1bf-4865-8903-b15673d05642"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from tqdm import trange\n",
        "\n",
        "args = {\n",
        "    'dataset': '',\n",
        "    'hidden_dim': 40,\n",
        "    'latent_dim': 5,\n",
        "    'batch_size': train_data.count_nonzero(),\n",
        "    'beta': None,\n",
        "    'gamma': 1,\n",
        "    'lr': 1e-3,\n",
        "    'n_epochs': 150,\n",
        "    'dropout_rate': 0.8,\n",
        "    'print_step': 1,\n",
        "    'n_enc_epochs': 3,\n",
        "    'n_dec_epochs': 1,\n",
        "    'not_alternating': True,\n",
        "}\n",
        "\n",
        "user_losses = []\n",
        "movie_losses = []\n",
        "mse_losses = []\n",
        "\n",
        "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
        "    yield Batch(device, [], data_in, data_out)\n",
        "\n",
        "\n",
        "class Batch:\n",
        "    def __init__(self, device, idx, data_in, data_out=None):\n",
        "        self._device = device\n",
        "        self._idx = idx\n",
        "        self._data_in = data_in\n",
        "        self._data_out = data_out\n",
        "        \n",
        "    def get_ratings(self, is_out=False):\n",
        "        data = self._data_in\n",
        "        return data\n",
        "    \n",
        "    def get_ratings_to_dev(self, is_out=False):\n",
        "        ratings = self.get_ratings(is_out)\n",
        "        return torch.Tensor(ratings.toarray()).to(self._device)\n",
        "\n",
        "\n",
        "def evaluate(model, data_in, data_out, RMSE_data, metrics, data_mn, data_mx, samples_perc_per_epoch=1, batch_size=500):\n",
        "    metrics = deepcopy(metrics)\n",
        "    model.eval()\n",
        "    \n",
        "    for m in metrics:\n",
        "        m['score'] = []\n",
        "    \n",
        "    for batch in generate(batch_size=batch_size,\n",
        "                          device=device,\n",
        "                          data_in=data_in,\n",
        "                          data_out=data_out,\n",
        "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
        "                         ):\n",
        "        \n",
        "        ratings = batch.get_ratings_to_dev()\n",
        "        ratings_pred = model(ratings, R_train, R_train_tensor, R_train_T, R_train_tensor_T, calculate_loss=False) * (data_mx - data_mn) + data_mn\n",
        "        for m in metrics:\n",
        "            x = m['metric'](ratings_pred, RMSE_data)\n",
        "            m['score'].append(x.cpu().detach())\n",
        "    for m in metrics:\n",
        "        m['score'] = np.sqrt(np.sum(m['score']) / data_in.count_nonzero())\n",
        "        \n",
        "    return [x['score'] for x in metrics]\n",
        "\n",
        "\n",
        "def run(model, opts, train_data, batch_size, n_epoch, n_epochs, beta, gamma, dropout_rate):\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch in generate(batch_size=batch_size, device=device, data_in=train_data, shuffle=False):\n",
        "            ratings = batch.get_ratings_to_dev()\n",
        "\n",
        "            for optimizer in opts:\n",
        "                optimizer.zero_grad()\n",
        "              \n",
        "            loss, user_loss, movie_loss, mse = model(ratings, R_train, R_train_tensor, R_train_T, R_train_tensor_T, beta=beta, gamma=gamma, dropout_rate=dropout_rate, n_epoch=n_epoch)\n",
        "            loss.backward()\n",
        "\n",
        "            user_losses.append(user_loss)\n",
        "            movie_losses.append(movie_loss)\n",
        "            mse_losses.append(mse)\n",
        "            # print('Epoch:', n_epoch, 'user_loss=', f'{user_loss:.4f}', 'movie_loss', f'{movie_loss:.4f}', 'mse_loss', f'{mse:.4f}')\n",
        "\n",
        "\n",
        "            for optimizer in opts:\n",
        "                optimizer.step()\n",
        "\n",
        "\n",
        "model_kwargs = {\n",
        "    'hidden_dim': args['hidden_dim'],\n",
        "    'latent_dim': args['latent_dim'],\n",
        "    'input_dim_users': train_data.shape[0],\n",
        "    'input_dim_movies': train_data.shape[1],\n",
        "    'device': device\n",
        "}\n",
        "metrics = [{'metric': RMSE}]\n",
        "\n",
        "best_valid = np.inf\n",
        "train_scores, valid_scores = [], []\n",
        "\n",
        "model = DualVAE(**model_kwargs).to(device)\n",
        "batch = Batch(device, [], train_data, train_data)\n",
        "ratings = batch.get_ratings_to_dev()\n",
        "model.pmf_init(R_train, R_train_tensor, R_train_T, R_train_tensor_T)\n",
        "model_best = DualVAE(**model_kwargs).to(device)\n",
        "\n",
        "learning_kwargs = {\n",
        "    'model': model,\n",
        "    'train_data': train_data,\n",
        "    'batch_size': args['batch_size'],\n",
        "    'beta': args['beta'],\n",
        "    'gamma': args['gamma']\n",
        "}\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(set(model.parameters()), lr=args['lr'], weight_decay=1)\n",
        "import time\n",
        "for epoch in range(args['n_epochs']):\n",
        "    t = time.time()\n",
        "\n",
        "    run(opts=[optimizer], n_epoch=epoch, n_epochs=1, dropout_rate=args['dropout_rate'], **learning_kwargs)\n",
        "\n",
        "    train_scores.append(\n",
        "        evaluate(model, train_data, train_data, RMSE_train, metrics, train_mn, train_mx, 1, batch_size=args['batch_size'])[0]\n",
        "    )\n",
        "    valid_scores.append(\n",
        "        evaluate(model, test_data, test_data, RMSE_test, metrics, test_mn, test_mx, 1, batch_size=args['batch_size'])[0]\n",
        "    )\n",
        "    if valid_scores[-1] < best_valid:\n",
        "      best_valid = valid_scores[-1]\n",
        "      model_best.load_state_dict(deepcopy(model.state_dict()))\n",
        "    if epoch % args['print_step'] == 0:\n",
        "      print('Time:', time.time() - t)\n",
        "      print('Epoch:', epoch, 'train_RMSE=', f'{train_scores[-1]:.4f}', 'test_RMSE', f'{valid_scores[-1]:.4f}', 'min_test_RMSE', f'{min(valid_scores):.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time: 12.8509361743927\n",
            "Epoch: 0 train_RMSE= 2.1900 test_RMSE 2.2555 min_test_RMSE 2.2555\n",
            "Time: 12.794533252716064\n",
            "Epoch: 1 train_RMSE= 1.3666 test_RMSE 1.4309 min_test_RMSE 1.4309\n",
            "Time: 12.831523656845093\n",
            "Epoch: 2 train_RMSE= 1.2706 test_RMSE 1.3349 min_test_RMSE 1.3349\n",
            "Time: 12.85643482208252\n",
            "Epoch: 3 train_RMSE= 1.2053 test_RMSE 1.2710 min_test_RMSE 1.2710\n",
            "Time: 12.741624593734741\n",
            "Epoch: 4 train_RMSE= 1.1595 test_RMSE 1.2244 min_test_RMSE 1.2244\n",
            "Time: 12.617982625961304\n",
            "Epoch: 5 train_RMSE= 1.1223 test_RMSE 1.1855 min_test_RMSE 1.1855\n",
            "Time: 12.678699970245361\n",
            "Epoch: 6 train_RMSE= 1.0943 test_RMSE 1.1561 min_test_RMSE 1.1561\n",
            "Time: 12.656322956085205\n",
            "Epoch: 7 train_RMSE= 1.0757 test_RMSE 1.1376 min_test_RMSE 1.1376\n",
            "Time: 12.782132863998413\n",
            "Epoch: 8 train_RMSE= 1.0600 test_RMSE 1.1193 min_test_RMSE 1.1193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-41fd672fc6da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dropout_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlearning_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     train_scores.append(\n",
            "\u001b[0;32m<ipython-input-6-41fd672fc6da>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, opts, train_data, batch_size, n_epoch, n_epochs, beta, gamma, dropout_rate)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_train_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_train_tensor_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-abe04102e474>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ratings, R_train, R_train_tensor, R_train_T, R_train_tensor_T, userFeatures, beta, gamma, dropout_rate, calculate_loss, n_epoch)\u001b[0m\n\u001b[1;32m    158\u001b[0m             mu_movies, z_j, movie_loss = self.movies_VAE(ratings.T, beta=beta, gamma=gamma, dropout_rate=dropout_rate,\n\u001b[1;32m    159\u001b[0m                                                          calculate_loss=calculate_loss, n_epoch=n_epoch)\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf_estimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_movies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_train_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_train_tensor_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;31m# y_hat = self.U @ self.V.T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-abe04102e474>\u001b[0m in \u001b[0;36mpmf_estimate\u001b[0;34m(self, mu_users, mu_movies, R_train, R_train_tensor, R_train_T, R_train_tensor_T)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0mv_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_conv_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mv_dev_sm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv_dev_sm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_u\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_eye\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}